{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tasks\n",
    "### 1: Form training and testing periods as discussed (2 test periods, last 2*365 days). Be careful tp scale data only based on data from the training set. You may use (from Python) MinMaxScaler, standardisation (StandardScaler) or the default scaler in Python's elastic net.\n",
    "### 2:Estimate the current flu rates by training an elastic net model. Use a Pearson correlation filter (r > 0.3) on the training data to reduce the amount of queries prior to training an elastic net (reminder: not all 1000 queries I provided are related to flu!). Report performance on the two test sets using three metrics: mean absolute error, root mean squared error and Pearson's correlation.\n",
    "### 3: If there is time, begin work on traditional forecasting models (you've identified seasonal ARIMA and Hult-Winters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from statsmodels.nonparametric import smoothers_lowess\n",
    "from pandas import Series, DataFrame\n",
    "from patsy import dmatrices\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "# initialize the plotting sizes\n",
    "# set size\n",
    "plt.rc('figure', figsize=(10, 5))\n",
    "# subplots size\n",
    "fizsize_with_subplots = (10, 10)\n",
    "# histogram size\n",
    "bin_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "dates = pd.read_csv('data/dates.csv',header=None)\n",
    "queries = pd.read_csv('data/queries.csv',header=None)\n",
    "X = pd.read_csv('data/X.csv',header=None)\n",
    "y = pd.read_csv('data/y.csv',header=None)\n",
    "\n",
    "X.columns = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (4383, 500)   y shape:  (4383, 1)\n",
      "train1 X: (4018, 500)  train1 y: (4018, 1)  Test1 X shape: (365, 500)  Test1 y shape: (365, 1)\n",
      "train2 X: (3653, 500)  train2 y: (3653, 1)  Test2 X: (730, 500)  Test2 y: (730, 1)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing and train-test split\n",
    "# this data is well formed with no missing value and other symbols or labels that are non numerical.\n",
    "# splitting the data into train and test\n",
    "\n",
    "# Here we want the first 500 queries\n",
    "X = X.iloc[:, 0: 500]\n",
    "\n",
    "# first with the last year as test\n",
    "test_size =365\n",
    "length = X.shape[0]\n",
    "train1_X = X[0:length-test_size]\n",
    "train1_y = y[0:length-test_size]\n",
    "test1_X = X[length-test_size:]\n",
    "test1_y = y[length-test_size:]\n",
    "\n",
    "# second with the last 2 year as test\n",
    "test_size2 =365*2\n",
    "train2_X = X[0:length-test_size2]\n",
    "train2_y = y[0:length-test_size2]\n",
    "test2_X = X[length-test_size2:]\n",
    "test2_y = y[length-test_size2:]\n",
    "\n",
    "print('X shape: ',X.shape,'  y shape: ',y.shape)\n",
    "print('train1 X:',train1_X.shape,' train1 y:',train1_y.shape,' Test1 X shape:',test1_X.shape, ' Test1 y shape:',test1_y.shape)\n",
    "print('train2 X:',train2_X.shape,' train2 y:',train2_y.shape,' Test2 X:',test2_X.shape,' Test2 y:',test2_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 1), (500,), ('flu',), ('symptoms of kidney infection',))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 4018\n",
    "corrs = np.zeros((X.shape[1],1))\n",
    "for i in range(0,X.shape[1]):\n",
    "    if X.sum(axis=1)[i] == 0:\n",
    "        corrs[i] = 0\n",
    "    else:\n",
    "        corrs[i] = np.corrcoef(X.iloc[0:train_size,i],y.iloc[0:train_size,0])[0,1]\n",
    "        \n",
    "corrs.shape,X.columns.shape,X.columns[0],X.columns[499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_filter(df_X,df_Xtest,corrs,threshold):\n",
    "    X = df_X.copy()\n",
    "    X_test = df_Xtest.copy()\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    for i in range(0,500):\n",
    "            if corrs[i,0] < threshold: \n",
    "                #print(X.columns[i])\n",
    "                colname = df_X.columns[i]\n",
    "                del X[colname] # deleting the column from the dataset\n",
    "                del X_test[colname] \n",
    "\n",
    "    return X,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_X0,test1_X0 = corr_filter(train1_X,test1_X,corrs,0.1)\n",
    "train1_X1,test1_X1 = corr_filter(train1_X,test1_X,corrs,0.2)\n",
    "train1_X2,test1_X2 = corr_filter(train1_X,test1_X,corrs,0.3)\n",
    "train1_X3,test1_X3 = corr_filter(train1_X,test1_X,corrs,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.1, we select feature number:  310\n",
      "For correlation filter r>=0.2, we select feature number:  212\n",
      "For correlation filter r>=0.3, we select feature number:  150\n",
      "For correlation filter r>=0.4, we select feature number:  103\n"
     ]
    }
   ],
   "source": [
    "print('For correlation filter r>=0.1, we select feature number: ',train1_X0.shape[1])\n",
    "print('For correlation filter r>=0.2, we select feature number: ',train1_X1.shape[1])\n",
    "print('For correlation filter r>=0.3, we select feature number: ',train1_X2.shape[1])\n",
    "print('For correlation filter r>=0.4, we select feature number: ',train1_X3.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defien the mearure matrics, MAE, RMSE, CORR\n",
    "# define three metrics: mean absolute error, root mean squared error and Pearson's correlation.\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# mae = mean_absolute_error(y_actual, y_pred)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "# rmse = sqrt(mean_squared_error(y_actual, y_pred))\n",
    "\n",
    "# np.correcoef returns Pearson product-moment correlation coefficients\n",
    "def pearson_r(x,y):   \n",
    "    corr_mat = np.corrcoef(x,y)\n",
    "    return corr_mat[0,1]\n",
    "# r = pearson_r(y_actual,y_pred)\n",
    "\n",
    "\n",
    "# Generalise the function for convinient tuning\n",
    "def eNet(a,l,train_X,train_y,test_X,test_y):\n",
    "    \n",
    "    # scaling and modeling\n",
    "    scalerX = StandardScaler()\n",
    "    scalerX.fit(train_X)\n",
    "    train_X = scalerX.transform(train_X)\n",
    "    test_X = scalerX.transform(test_X)\n",
    "    \n",
    "    scalery = StandardScaler() \n",
    "    scalery.fit(train_y) \n",
    "    train_y = scalery.transform(train_y) \n",
    "    #test_y = scalery.transform(test_y)\n",
    "    \n",
    "    #alpha = a\n",
    "    #l1_ratio = l\n",
    "    enet = ElasticNet(alpha=a, copy_X=True, fit_intercept=True, l1_ratio=l,\n",
    "      max_iter=10000, normalize=False, positive=False, precompute=False,\n",
    "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "    enet.fit(train_X,train_y)\n",
    "    \n",
    "    print('Nonzero weights: %d from %d' % (len(np.nonzero(enet.coef_)[0]), len(enet.coef_)))\n",
    "    y_pred1 = enet.predict(test_X)\n",
    "    \n",
    "    y_pred1 = scalery.inverse_transform(y_pred1)\n",
    "\n",
    "    #print(y_pred1.shape,test_y[:,0].shape)\n",
    "    \n",
    "    mae1 = mean_absolute_error(test_y, y_pred1)\n",
    "    print('The mean absolute error is: ',mae1)\n",
    "\n",
    "    rmse1 = sqrt(mean_squared_error(test_y, y_pred1))\n",
    "    print('The root mean squared error is: ',rmse1)\n",
    "    \n",
    "    corr_y = test_y.copy()\n",
    "    corr_y['y_act'] = test_y\n",
    "    corr_y['y_pred']= y_pred1\n",
    "    print('The correlation is: ',np.corrcoef(corr_y['y_act'],corr_y['y_pred'])[0,1])\n",
    "    \n",
    "    total = mae1+rmse1- np.corrcoef(corr_y['y_act'],corr_y['y_pred'])[0,1]\n",
    "    print(total)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.1, 310 features are selected\n",
      "alpha: 0.01  L1-ratio: 0.4\n",
      "Nonzero weights: 182 from 310\n",
      "The mean absolute error is:  2.6303552248996027\n",
      "The root mean squared error is:  3.247224679968285\n",
      "The correlation is:  0.917503448908422\n",
      "4.960076455959467\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [0.01]\n",
    "para_l=[0.4]\n",
    "print('For correlation filter r>=0.1, 310 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train1_X0,train1_y,test1_X0,test1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.2, 212 features are selected\n",
      "alpha: 1.09  L1-ratio: 0.3\n",
      "Nonzero weights: 19 from 212\n",
      "The mean absolute error is:  4.534601429940703\n",
      "The root mean squared error is:  5.02256044172397\n",
      "The correlation is:  0.8326618376341202\n",
      "8.724500034030552\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [1.09]\n",
    "para_l=[0.3]\n",
    "print('For correlation filter r>=0.2, 212 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train1_X1,train1_y,test1_X1,test1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.3, 150 features are selected\n",
      "alpha: 0.01  L1-ratio: 0.01\n",
      "Nonzero weights: 149 from 150\n",
      "The mean absolute error is:  2.6891096524313056\n",
      "The root mean squared error is:  3.2349801041664223\n",
      "The correlation is:  0.9239579225739746\n",
      "5.000131834023754\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [0.01]\n",
    "para_l=[0.01]\n",
    "print('For correlation filter r>=0.3, 150 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train1_X2,train1_y,test1_X2,test1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.4, 103 features are selected\n",
      "alpha: 0.01  L1-ratio: 0.01\n",
      "Nonzero weights: 102 from 103\n",
      "The mean absolute error is:  2.8430629849367426\n",
      "The root mean squared error is:  3.289934057935316\n",
      "The correlation is:  0.9152612670397793\n",
      "5.2177357758322795\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [0.01]\n",
    "para_l=[0.01]\n",
    "print('For correlation filter r>=0.4, 103 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train1_X3,train1_y,test1_X3,test1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the last 2 years as the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 1), (500,), ('flu',), ('symptoms of kidney infection',))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 3653\n",
    "corrs2 = np.zeros((X.shape[1],1))\n",
    "for i in range(0,X.shape[1]):\n",
    "    if X.sum(axis=1)[i] == 0:\n",
    "        corrs2[i] = 0\n",
    "    else:\n",
    "        corrs2[i] = np.corrcoef(X.iloc[0:train_size,i],y.iloc[0:train_size,0])[0,1]\n",
    "        \n",
    "corrs2.shape,X.columns.shape,X.columns[0],X.columns[499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_X0,test2_X0 = corr_filter(train2_X,test2_X,corrs2,0.1)\n",
    "train2_X1,test2_X1 = corr_filter(train2_X,test2_X,corrs2,0.2)\n",
    "train2_X2,test2_X2 = corr_filter(train2_X,test2_X,corrs2,0.3)\n",
    "train2_X3,test2_X3 = corr_filter(train2_X,test2_X,corrs2,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>0.1, we select feature number:  303\n",
      "For correlation filter r>0.2, we select feature number:  215\n",
      "For correlation filter r>0.3, we select feature number:  154\n",
      "For correlation filter r>0.4, we select feature number:  105\n"
     ]
    }
   ],
   "source": [
    "print('For correlation filter r>0.1, we select feature number: ',train2_X0.shape[1])\n",
    "print('For correlation filter r>0.2, we select feature number: ',train2_X1.shape[1])\n",
    "print('For correlation filter r>0.3, we select feature number: ',train2_X2.shape[1])\n",
    "print('For correlation filter r>0.4, we select feature number: ',train2_X3.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.1, 303 features are selected\n",
      "alpha: 0.01  L1-ratio: 0.26\n",
      "Nonzero weights: 212 from 303\n",
      "The mean absolute error is:  2.523319760568521\n",
      "The root mean squared error is:  3.266459986228576\n",
      "The correlation is:  0.875528619721812\n",
      "4.914251127075285\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [0.01]\n",
    "para_l=[0.26]\n",
    "print('For correlation filter r>=0.1, 303 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train2_X0,train2_y,test2_X0,test2_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.2, 215 features are selected\n",
      "alpha: 0.01  L1-ratio: 0.12\n",
      "Nonzero weights: 183 from 215\n",
      "The mean absolute error is:  2.627647539596773\n",
      "The root mean squared error is:  3.349693597355846\n",
      "The correlation is:  0.8704305105179881\n",
      "5.106910626434631\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [0.01]\n",
    "para_l=[0.12]\n",
    "print('For correlation filter r>=0.2, 215 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train2_X1,train2_y,test2_X1,test2_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.3, 154 features are selected\n",
      "alpha: 0.01  L1-ratio: 0.01\n",
      "Nonzero weights: 151 from 154\n",
      "The mean absolute error is:  2.6122891594499604\n",
      "The root mean squared error is:  3.3028834261242697\n",
      "The correlation is:  0.8827328622776955\n",
      "5.032439723296535\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [0.01]\n",
    "para_l=[0.01]\n",
    "print('For correlation filter r>=0.3, 154 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train2_X2,train2_y,test2_X2,test2_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For correlation filter r>=0.4, 105 features are selected\n",
      "alpha: 0.01  L1-ratio: 0.01\n",
      "Nonzero weights: 105 from 105\n",
      "The mean absolute error is:  2.8293170011843682\n",
      "The root mean squared error is:  3.365114670311403\n",
      "The correlation is:  0.8937143146905017\n",
      "5.3007173568052695\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "para = [0.01]\n",
    "para_l=[0.01]\n",
    "print('For correlation filter r>=0.4, 105 features are selected')\n",
    "for a in para:\n",
    "    for l in para_l:\n",
    "        print('alpha:',a,' L1-ratio:',l)\n",
    "        print(eNet(a,l,train2_X3,train2_y,test2_X3,test2_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
